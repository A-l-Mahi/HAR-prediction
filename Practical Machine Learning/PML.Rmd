---
title: "Practical Machine Learning"
author: "Mahi Aminu Aliyu"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  html_document: default
  md_documents: default
---
# Human Activity Recognition

The goal of this project is to qunatify how well people perform activity using data from accelerometers on the belt, forearm, arm and dumbbel of 6 participants.  
The accelerometers masure the participants perform 5 activities (sitting-down, standing-up, standing, walking, and sitting) as A, B, C, D, and E in the classe variable.  

---

```{r message = FALSE, echo = F}
library(dplyr); library(caret); library(ggplot2); require(parallel); require(doParallel); require(dplyr)
set.seed(333)
```


# [Dataset](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har#sbia_paper_section)

The dataset proposed with 5 classe (sitting-down, standing-up, standing, walking, and sitting) collected on 8 hours of activities of 4 healthy subjects. We also established a baseline performance index.

>Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.

### Reading dataset
```{r}
if(!file.exists("dataset/pml-training.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              destfile = "dataset/pml-training.csv")
}

if(!file.exists("dataset/pml-testing.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              destfile = "dataset/pml-testing.csv")
}

pml <- read.csv("dataset/pml-training.csv")

```

## Data

```{r}
dim(pml)
```
The dataset has 19622 rows and 160 variables.


### Removing NAs and empty characters

Here we get the percentage of variables that are Na or empty characters and discard them

First thing first we check for NAs in the data set
```{r}
table(colSums(is.na(pml)))
```
is.na gets the total NAs an non NAs for each column 
93 column are non missing values will 67 columns have 19216 missing values suggest 

Base on the table we can observe that columns with zero NAs are 93 and
Column with 19216 NAs are 67 we can also noticed that there is uniform NAs
in each column with NAs, with total rows of 19622 we got 406 NAs
That's we have only 2% of non NAs in columns with NAs We will discard the tables because we wont get anything meaningful.



```{r}
table(colSums(pml == ""))
```
33 columns have 19216 for empty strings only 60 columns

there we discard all the missing columns and empty columns
```{r}
#discard empty strings and NAs


df <-  pml[, colSums(is.na(pml)) == 0]

df <- df[,colSums(df == "") == 0] 
```

```{r}

1 - (length(names(df)) / length(names(pml)))

```
now we can see that 62.5 of the data are either Nas of Empty characters


```{r}
dim(df)
```
we are now down to 60 columns

For this prediction task variables such as X, timestamp, window and user_name
are irrelevant to our aim, an we will prepare to use data that are closely related to out outcome.

```{r}
df <- df[, -c(which(grepl("^X|timestamp|window|user_name", names(df))))]
dim(df)
# And lastly 53 columns
```

## preProcessing

The outcome will be classe (i.e the manner of activities)

# Considering the enough data set we have, lets slice the training data set to subTrain and validation set

```{r}

subTrain <- createDataPartition(y = df$classe, p = 0.80, list = F)

subTraining <- df[subTrain, ]
validation <- df[-subTrain, ]
```



```{r}
subTraining$classe <- as.factor(subTraining$classe)
ggplot(df, aes(x = classe)) +
  stat_count()

#df[, -53] <- lapply(df[, -53], as.numeric)

#sam <- sample(length(nrow(df), 500))

#df <- pml[sam ,]

```
classe A has more instance than other classe, hence we assign wighths to the imbalance dataset before training.
We will use inverse class frequencies method to the determine the class weights
```{r}
weights <- table(df$classe)
class_weights <- 1 / weights

class_weights <- class_weights / sum(class_weights)

print("class weights:")
print(class_weights)
```


#tuning parameteres

```{r}
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 5,
  classProbs = T,
  verboseIter = F,
  allowParallel = T,
  summaryFunction = multiClassSummary

)
```


# Training

we are going to use three classification algorithms for namely:
  1. Stochastic Gradient Boosting Machine: Is a boosted machine learning algorithm
  2. Random Forest, and: A highly accurate
  3. Support Vector Machine: base on the linearity 
  
  

```{r}
set.seed(333)
modFitGBM <- train(classe ~ ., method = "gbm", data = subTraining,
                   trControl = fitControl,
                   verbose = F

)
modFitGBM$results
```
The models suggest we will get the best accuracy using interaction.depth of 3 and n.trees of 150 while shrinkage and n.minovsinnode were held at constant 0.1 and 10 respectively.
```{r}
set.seed(333)

modFitFor <- train(classe ~., method = "rf", data = subTraining,
                   trContol = fitControl,
                   class.weights = class_weights

)
modFitFor$results
```
the final value for mtry was 27 which gives us the optimal model

```{r}
set.seed(333)

modFitSVM <- train(classe ~., method = "svmLinear2", data = subTraining,
                  trControl = fitControl,
                  class.weights = class_weights

)
modFitSVM$results

```
cost = 1 appears to five the optimal model


We have not included class weights for gredient boosting machine, because NAs are produce.

# Prediction
Using the validation dataset will predict and evaluate our algorithm 

```{r}
trellis.par.set(caretTheme())
ggplot(modFitGBM, metric = "Kappa")
ggplot(modFitFor, metric = "Kappa")
ggplot(modFitSVM, metric = "Kappa")
```


```{r}
preLogit <- predict(modFitGBM, newdata = validation)
preFor <- predict(modFitFor, newdata = validation)
preSVM <- predict(modFitSVM,newdata = validation)

```

# Confusion Matrics

```{r}
matricsLogit <- confusionMatrix(preLogit, as.factor(validation$classe))
matricsFor <- confusionMatrix(preFor, as.factor(validation$classe))
matricsSVM <- confusionMatrix(preSVM, as.factor(validation$classe))

cat("Metrics for Stochasic Boosting Gredient Machine:")
print(matricsLogit$table)
print(round(matricsLogit$overall, 6))

cat("\n")

cat("Metrics for Random Forest:")
print(matricsFor$table)
print(round(matricsFor$overall, 6))

cat("\n")

cat("Metrics for Support Vector Machine:")
print(matricsSVM$table)
print(round(matricsSVM$overall, 6))

```
